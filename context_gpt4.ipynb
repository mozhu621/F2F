{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/anaconda/envs/easycontext/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 7.62k/7.62k [00:00<00:00, 17.9MB/s]\n",
      "Downloading data: 100%|██████████| 14.5M/14.5M [00:00<00:00, 14.7MB/s]\n",
      "Downloading data: 100%|██████████| 1.82M/1.82M [00:00<00:00, 3.00MB/s]\n",
      "Generating train split: 100%|██████████| 87599/87599 [00:00<00:00, 717028.42 examples/s]\n",
      "Generating validation split: 100%|██████████| 10570/10570 [00:00<00:00, 668735.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique contexts: 18891\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# 加载SQuAD数据集\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# 使用字典来存储首次出现的context和对应的完整example\n",
    "unique_examples = {}\n",
    "\n",
    "for example in squad['train']:\n",
    "    context = example['context']\n",
    "    if context not in unique_examples:\n",
    "        unique_examples[context] = example\n",
    "\n",
    "# 输出不同context的数量\n",
    "print(f\"Total number of unique contexts: {len(unique_examples)}\")\n",
    "\n",
    "# 将这些唯一的examples保存到JSON文件中\n",
    "with open('unique_contexts.json', 'w') as f:\n",
    "    json.dump(list(unique_examples.values()), f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def generate_unique_string(length=12, seen=set()):\n",
    "    \"\"\"\n",
    "    Generate a unique string of specified length that includes at least one lowercase letter, one uppercase letter,\n",
    "    one digit, and one special character from the set {@, $, %, #, &}.\n",
    "    Ensures that no string is repeated using a 'seen' set.\n",
    "    \"\"\"\n",
    "    # Define the character pools\n",
    "    lower = string.ascii_lowercase\n",
    "    upper = string.ascii_uppercase\n",
    "    digits = string.digits\n",
    "    special = \"@$%#&\"\n",
    "    \n",
    "    while True:\n",
    "        # Ensure at least one character from each required category\n",
    "        characters = [\n",
    "            random.choice(lower),\n",
    "            random.choice(upper),\n",
    "            random.choice(digits),\n",
    "            random.choice(special)\n",
    "        ]\n",
    "        \n",
    "        # Fill the rest of the string with a random choice from the combined pool\n",
    "        combined_pool = lower + upper + digits + special\n",
    "        characters.extend(random.choices(combined_pool, k=length - 4))\n",
    "        \n",
    "        # Shuffle to ensure the characters are not in the same order every time\n",
    "        random.shuffle(characters)\n",
    "        \n",
    "        # Join the list into a string\n",
    "        result = ''.join(characters)\n",
    "        \n",
    "        # Check if this result has been generated before\n",
    "        if result not in seen:\n",
    "            seen.add(result)\n",
    "            return result\n",
    "\n",
    "# Example usage\n",
    "seen_strings = set()\n",
    "for _ in range(10):  # Generate 10 unique strings\n",
    "    print(generate_unique_string(seen=seen_strings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def merge_json_files(folder_path, output_file):\n",
    "    \"\"\" Merges multiple JSON files from a specified folder into a single JSON file \"\"\"\n",
    "    merged_data = []\n",
    "    \n",
    "    # List all files in the given folder\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "    files.sort()  # Optional, sorts the files by name if needed for ordered merging\n",
    "    \n",
    "    # Iterate over each file and append its content to merged_data\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        with open(file_path, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "            merged_data.extend(data)  # Append the data from each file to the list\n",
    "    \n",
    "    # Write the merged data to a new JSON file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(merged_data, outfile, indent=4)\n",
    "    \n",
    "    print(f\"All data has been merged into {output_file}.\")\n",
    "\n",
    "# Usage\n",
    "folder_path = 'merge_data'  # Folder containing the JSON files\n",
    "output_file = 'merged_data.json'  # Name of the output file\n",
    "merge_json_files(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "def count_elements_in_json(filepath):\n",
    "    \"\"\"\n",
    "    Loads a JSON file and counts the number of top-level elements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            if isinstance(data, dict):\n",
    "                # If the top level is a dictionary, count the number of keys\n",
    "                return len(data)\n",
    "            elif isinstance(data, list):\n",
    "                # If the top level is a list, count the number of items\n",
    "                return len(data)\n",
    "            else:\n",
    "                # Other JSON structures are not expected\n",
    "                return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "filepath = 'version_10_test_4k.json'  # Replace with your actual file path\n",
    "element_count = count_elements_in_json(filepath)\n",
    "if element_count is not None:\n",
    "    print(f\"The number of top-level elements in the JSON file is: {element_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "\n",
    "global data_index\n",
    "dataset = load_dataset(\"DKYoon/SlimPajama-6B\", split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_unique_string():\n",
    "    \"\"\" Generate a unique string based on hashing \"\"\"\n",
    "    unique_id = sha256(os.urandom(48)).hexdigest()[:16]\n",
    "    return unique_id\n",
    "\n",
    "import random\n",
    "def modify_gpt_answer(gpt_answer, correct_answer_id, question_index):\n",
    "    \"\"\" Modifies the GPT_answer to include the correct answer ID at the appropriate position \"\"\"\n",
    "    insert_text = f\"\\n\\n\\n### Retrieval: The answer to question {question_index} can be found in ID {correct_answer_id}.\\n\\n\"\n",
    "    analysis_index = gpt_answer.find(\"### Analysis:\")\n",
    "    if analysis_index != -1:\n",
    "        part_before = gpt_answer[:analysis_index + len(\"### Analysis:\")]\n",
    "        part_after = gpt_answer[analysis_index + len(\"### Analysis:\"):]\n",
    "        return  insert_text + part_before  + part_after\n",
    "    else:\n",
    "        return  insert_text  +  gpt_answer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_ctxs_to_item(base_item, items, data_index):\n",
    "    \"\"\" Adds 'ctxs' key to a group of items and updates GPT_answer \"\"\"\n",
    "    ctxs = []\n",
    "    questions = []\n",
    "    questions_ctx = []\n",
    "    gpt_answers = []\n",
    "    answer_map = {}\n",
    "    # Process each item to create questions, answers, and modify GPT_answer\n",
    "    for idx, item in enumerate(items):\n",
    "        context_id = generate_unique_string()\n",
    "        questions.append(f\"Question {idx + 1}: {item['question']}\")\n",
    "        if idx <= 2:\n",
    "            question_id = generate_unique_string()\n",
    "            ctxs.append({\n",
    "                \"ID\": question_id,\n",
    "                \"text\": item['question'],\n",
    "                \"hasAnswer\": 'question',\n",
    "                \"GPT_answer\":item['GPT_answer'],\n",
    "                \"context_id\": context_id\n",
    "            })\n",
    "            ctxs.append({\n",
    "                \"ID\": context_id,\n",
    "                \"text\": item['context'],\n",
    "                \"hasAnswer\": True\n",
    "            })\n",
    "            # Store answer mapping for modification later\n",
    "        else:\n",
    "            ctxs.append({\n",
    "                \"ID\": context_id,\n",
    "                \"text\": item['context'],\n",
    "                \"hasAnswer\": False\n",
    "            })\n",
    "    # Add random contexts from dataset\n",
    "    data_index = add_contexts(dataset, ctxs, data_index)\n",
    "\n",
    "    # Shuffle the list of contexts to ensure randomness\n",
    "    random.shuffle(ctxs)\n",
    "    # Reconstruct the mapping of question IDs to answer IDs after shuffling\n",
    "\n",
    "    for ctx in ctxs:\n",
    "        if ctx['hasAnswer'] == 'question':\n",
    "            gpt_answers.append(modify_gpt_answer(ctx['GPT_answer'],ctx['context_id'],ctx['ID']))\n",
    "            \n",
    "\n",
    "    base_item['GPT_answer'] = ' '.join(gpt_answers)\n",
    "    base_item['question'] = ' '.join(questions)\n",
    "    return ctxs, data_index\n",
    "\n",
    "\n",
    "def add_contexts(dataset, ctxs, data_index):\n",
    "    \"\"\" Adds contexts from the dataset while checking the token conditions \"\"\"\n",
    "    total_tokens = 0\n",
    "    max_tokens=12500\n",
    "    while total_tokens < max_tokens:\n",
    "        text = dataset[data_index]['text']\n",
    "        data_index = data_index+1\n",
    "        word_count = len(text.split())\n",
    "        if 80 <= word_count <= 500:\n",
    "            context_id = generate_unique_string()\n",
    "            ctxs.append({\"ID\": context_id, \"text\": text, \"hasAnswer\": False})\n",
    "            total_tokens += word_count  # Update the token count assuming 1 token per word\n",
    "    return data_index\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(filepath):\n",
    "    \"\"\" Loads JSON data from a file \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from {filepath}: {e}\")\n",
    "        return []\n",
    "    \n",
    "def expand_and_enhance_data(data, data_index ):\n",
    "    \"\"\" Expands each item by 32 times and enhances with ctxs \"\"\"\n",
    "    enhanced_data = []\n",
    "    for item in data:\n",
    "        for _ in range(1):  # Duplicate each item 32 times\n",
    "            # Select 8 other random items\n",
    "            other_items = random.sample(data, 19)\n",
    "            other_items.insert(0, item)  # Include the current item as the first item\n",
    "            new_item = item.copy()  # Make a copy of the item\n",
    "            new_item['ctxs'],data_index = add_ctxs_to_item(new_item, other_items,data_index)\n",
    "            enhanced_data.append(new_item)\n",
    "    random.shuffle(enhanced_data)  # Shuffle all items to mix them up\n",
    "    return enhanced_data, data_index\n",
    "\n",
    "def save_data(data, output_file):\n",
    "    \"\"\" Saves processed data into a JSON file \"\"\"\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# Main logic\n",
    "merged_file_path = 'merged_data.json'\n",
    "output_file = 'version_6_test_4k.json'\n",
    "merged_data = load_json_data(merged_file_path)\n",
    "merged_data = merged_data[20:4020]\n",
    "\n",
    "if merged_data:\n",
    "    data_index = 0\n",
    "    enhanced_data,data_index = expand_and_enhance_data(merged_data,data_index )\n",
    "    save_data(enhanced_data, output_file)\n",
    "    print(data_index)\n",
    "else:\n",
    "    print(\"Failed to load data or context data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from hashlib import sha256\n",
    "\n",
    "def generate_unique_string():\n",
    "    \"\"\" Generate a unique string based on hashing \"\"\"\n",
    "    unique_id = sha256(os.urandom(48)).hexdigest()[:24]\n",
    "    return unique_id\n",
    "\n",
    "def load_json_data(filepath):\n",
    "    \"\"\" Loads JSON data from a file \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "def modify_gpt_answer(gpt_answer, correct_answer_id, question_index):\n",
    "    \"\"\" Modifies the GPT_answer to include the correct answer ID at the appropriate position \"\"\"\n",
    "    insert_text = f\"\\n\\n### Retrieval: The answer to question {question_index + 1} can be found in ID {correct_answer_id}.\"\n",
    "    analysis_index = gpt_answer.find(\"### Analysis:\")\n",
    "    if analysis_index != -1:\n",
    "        part_before = gpt_answer[:analysis_index + len(\"### Analysis:\")]\n",
    "        part_after = gpt_answer[analysis_index + len(\"### Analysis:\"):]\n",
    "        return  insert_text + part_before  + part_after\n",
    "    else:\n",
    "        return  insert_text  + gpt_answer \n",
    "\n",
    "def add_ctxs_to_item(base_item, items, context_data):\n",
    "    \"\"\" Adds 'ctxs' key to a group of items and updates GPT_answer \"\"\"\n",
    "    ctxs = []\n",
    "    questions = []\n",
    "\n",
    "    gpt_answers = []\n",
    "\n",
    "    # Process each item to create questions, answers, and modify GPT_answer\n",
    "    for idx, item in enumerate(items):\n",
    "        # Generate a unique ID for each context, all will have answers\n",
    "        context_id = generate_unique_string()\n",
    "        questions.append(f\"Question {idx + 1}: {item['question']}\")\n",
    "        \n",
    "        gpt_answers.append(modify_gpt_answer(item['GPT_answer'], context_id, idx))\n",
    "        # Add current item's context with hasAnswer=True\n",
    "        ctxs.append({\n",
    "            \"ID\": context_id,\n",
    "            \"text\": item['context'],\n",
    "            \"hasAnswer\": True\n",
    "        })\n",
    "\n",
    "    # Add 80 random contexts from context_data without answers\n",
    "    sampled_contexts = random.sample(context_data, 90)\n",
    "    for context in sampled_contexts:\n",
    "        ctxs.append({\n",
    "            \"ID\": generate_unique_string(),\n",
    "            \"text\": context['context'],\n",
    "            \"hasAnswer\": False\n",
    "        })\n",
    "\n",
    "    # Shuffle the list of contexts to ensure randomness\n",
    "    random.shuffle(ctxs)\n",
    "    # Update the base item\n",
    "    base_item['question'] = ' '.join(questions)\n",
    "    base_item['GPT_answer'] = ' '.join(gpt_answers)\n",
    "    return ctxs\n",
    "\n",
    "\n",
    "def expand_and_enhance_data(data, context_data):\n",
    "    \"\"\" Expands each item by 32 times and enhances with ctxs \"\"\"\n",
    "    enhanced_data = []\n",
    "    for item in data:\n",
    "        for _ in range(15):  # Duplicate each item 32 times\n",
    "            # Select 8 other random items\n",
    "            other_items = random.sample(data, 9)\n",
    "            other_items.insert(0, item)  # Include the current item as the first item\n",
    "            new_item = item.copy()  # Make a copy of the item\n",
    "            new_item['ctxs'] = add_ctxs_to_item(new_item, other_items, context_data)\n",
    "            enhanced_data.append(new_item)\n",
    "    random.shuffle(enhanced_data)  # Shuffle all items to mix them up\n",
    "    return enhanced_data\n",
    "\n",
    "def save_data(data, output_file):\n",
    "    \"\"\" Saves processed data into a JSON file \"\"\"\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# Main logic\n",
    "merged_file_path = 'merged_data.json'\n",
    "context_file_path = 'unique_contexts.json'\n",
    "output_file = 'enhanced_data.json'\n",
    "\n",
    "merged_data = load_json_data(merged_file_path)\n",
    "context_data = load_json_data(context_file_path)\n",
    "merged_data = merged_data\n",
    "context_data = context_data[1000:]\n",
    "if merged_data and context_data:\n",
    "    enhanced_data = expand_and_enhance_data(merged_data, context_data)\n",
    "    save_data(enhanced_data, output_file)\n",
    "else:\n",
    "    print(\"Failed to load data or context data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#变为行\n",
    "import json\n",
    "\n",
    "# 打开并读取JSON文件\n",
    "with open('version_6_test_4k.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 检查data是否是列表类型，如果是，按行写入每个元素\n",
    "if isinstance(data, list):\n",
    "    with open('line_to_line.json', 'w', encoding='utf-8') as output:\n",
    "        for item in data:\n",
    "            json.dump(item, output)\n",
    "            output.write('\\n')\n",
    "else:\n",
    "    print(\"JSON root is not a list. Cannot save items line by line.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_1 = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "\n",
    "dataset_2 = load_dataset(\"DKYoon/SlimPajama-6B\", split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_criteria(example):\n",
    "    # 计算单词数量\n",
    "    question_words = len(example['question'].split())\n",
    "    response_words = len(example['response'].split())\n",
    "    \n",
    "    # 将question转换为小写并检查是否含有单词\"translate\"或\"translation\"\n",
    "    # question_lower = example['question'].lower()\n",
    "    # contains_translate = \"translate\" in question_lower or \"translation\" in question_lower\n",
    "    id_starts_with_cot = example['id'].startswith('cot')\n",
    "    # 判断单词数量是否都小于1000且question中不含\"translate\"和\"translation\"\n",
    "    # return 80 < question_words < 1000 and 80 < response_words < 1000 and not contains_translate and id_starts_with_cot\n",
    "    return  id_starts_with_cot and 20 < response_words and 20 < question_words\n",
    "\n",
    "# 应用过滤条件\n",
    "dataset_1 = dataset_1.filter(filter_criteria)\n",
    "\n",
    "# 随机打乱数据集\n",
    "dataset_1= dataset_1.shuffle()\n",
    "dataset_2 = dataset_2.shuffle()\n",
    "# 取出训练集\n",
    "filtered_dataset = dataset_1['train']\n",
    "dataset_1 = filtered_dataset\n",
    "# 检查过滤后的数据\n",
    "print(filtered_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modify_gpt_answer_end(gpt_answer, correct_answer_id, question_index):\n",
    "    \"\"\" Modifies the GPT_answer to include the correct answer ID at the appropriate position \"\"\"\n",
    "    insert_text = f\"### Retrieval: The answer to question {question_index} can be found in ID {correct_answer_id}.\"\n",
    "    analysis_index = gpt_answer.find(\"### Analysis:\")\n",
    "    if analysis_index != -1:\n",
    "        part_before = gpt_answer[:analysis_index + len(\"### Analysis:\")]\n",
    "        part_after = gpt_answer[analysis_index + len(\"### Analysis:\"):]\n",
    "        return  insert_text + part_before  + part_after\n",
    "    else:\n",
    "        return  insert_text  + gpt_answer \n",
    "\n",
    "def generate_unique_string():\n",
    "    \"\"\" Generate a unique string based on hashing \"\"\"\n",
    "    unique_id = sha256(os.urandom(48)).hexdigest()[:24]\n",
    "    return unique_id\n",
    "\n",
    "import random\n",
    "def word_len(example):\n",
    "    # 计算单词数量\n",
    "    lens = len(example.split())\n",
    "    return lens\n",
    "\n",
    "\n",
    "def add_ctxs_to_item(base_item, items,  data_index_1,data_index_2):\n",
    "    \"\"\" Adds 'ctxs' key to a group of items and updates GPT_answer \"\"\"\n",
    "    ctxs = []\n",
    "    questions = []\n",
    "    questions_ctxs = []\n",
    "    end_qa_pair= []\n",
    "    answer_map = {}\n",
    "    total_length = 0\n",
    "    # Process each item to create questions, answers, and modify GPT_answer\n",
    "    for idx, item in enumerate(items):\n",
    "        context_id = generate_unique_string()\n",
    "        question_id = generate_unique_string()\n",
    "        # middle answer \n",
    "        if idx < 10:\n",
    "            question_id = generate_unique_string()\n",
    "            ctxs.append({\n",
    "                \"ID\": context_id,\n",
    "                \"text\": item['context']+f\" ### Question:\"+ item['question'],\n",
    "                \"type\": 'mid',\n",
    "                \"Q_ID\": question_id,\n",
    "                \"GPT_answer\":item['GPT_answer'],\n",
    "            }) \n",
    "            total_length += word_len(item['context']+item['question']+item['GPT_answer'])\n",
    "        # last answer \n",
    "        elif idx < 20:\n",
    "            end_qa_pair.append(f\"\\nQuestion {idx-9}: {item['question']}\")\n",
    "            #  insert_text = f\"### Retrieval: The answer to question {question_index} can be found in ID {correct_answer_id}.\"\n",
    "            end_qa_pair.append(modify_gpt_answer_end(item['GPT_answer'], context_id, idx-9))\n",
    "            ctxs.append({\n",
    "                \"ID\": context_id,\n",
    "                \"text\": item['context'],\n",
    "                \"type\": 'end',\n",
    "            })\n",
    "            total_length += word_len(item['context']+item['question']+item['GPT_answer']+f\"### Retrieval: The answer to question can be found in ID\")\n",
    "        else:\n",
    "            ctxs.append({\n",
    "                \"ID\": context_id,\n",
    "                \"text\": item['context'],\n",
    "                \"type\": 'false'\n",
    "            })\n",
    "            total_length += word_len(item['context'])\n",
    "    # Add random contexts from dataset\n",
    "    data_index_1,total_length = add_contexts_Orca(dataset_1,ctxs,data_index_1, total_length)\n",
    "    data_index_2,total_length = add_contexts(dataset_2, ctxs, data_index_2, total_length)\n",
    "    print(total_length)\n",
    "    # Shuffle the list of contexts to ensure randomness\n",
    "    random.shuffle(ctxs)\n",
    "    # Reconstruct the mapping of question IDs to answer IDs after shuffling\n",
    "    for i, ctx in enumerate(ctxs):\n",
    "        if ctx.get('type') == 'mid':\n",
    "\n",
    "            insert_text = f\"Please answer the question ID: {ctx['ID']}, ### Answer: \"\n",
    "            Answer = ctx['GPT_answer']\n",
    "            \n",
    "            questions_ctxs.append({\n",
    "                \"ID\": ctx['Q_ID'],\n",
    "                \"text\": insert_text + Answer,\n",
    "                \"type\": 3 ,\n",
    "                \"location\" : i+1\n",
    "            })\n",
    "\n",
    "    offset = 0\n",
    "    for new_ctx in questions_ctxs:\n",
    "        start_position = new_ctx['location'] + offset\n",
    "        # 检查起始位置是否超出列表长度，如果超出，则设置插入位置为列表末尾\n",
    "        if start_position >= len(ctxs):\n",
    "            insert_position = len(ctxs)\n",
    "        else:\n",
    "            insert_position = random.randint(start_position, len(ctxs))\n",
    "        ctxs.insert(insert_position, new_ctx)\n",
    "        offset += 1  \n",
    "\n",
    "    base_item['GPT_answer'] = '\\n '.join(end_qa_pair)\n",
    "    base_item['question'] = ' '.join(questions)\n",
    "    return ctxs, data_index_1,data_index_2\n",
    "\n",
    "\n",
    "def add_contexts(dataset, ctxs, data_index,total_length):\n",
    "    \"\"\" Adds contexts from the dataset while checking the token conditions \"\"\"\n",
    "    total_tokens = total_length\n",
    "    max_tokens=19000\n",
    "    while total_tokens < max_tokens:\n",
    "        text = dataset[data_index]['text']\n",
    "        data_index = data_index+1\n",
    "        word_count = len(text.split())\n",
    "        if 200 <= word_count <= 1000:\n",
    "            context_id = generate_unique_string()\n",
    "            ctxs.append({\"ID\": context_id, \"text\": text, \"type\": False})\n",
    "            total_tokens += word_count  # Update the token count assuming 1 token per word\n",
    "    return data_index,total_tokens\n",
    "\n",
    "def add_contexts_Orca(dataset, ctxs, data_index,total_length):\n",
    "    \"\"\" Adds contexts from the dataset while checking the token conditions \"\"\"\n",
    "    total_tokens = total_length\n",
    "    num_QA = 0\n",
    "    max_QA = 30\n",
    "    while num_QA  < max_QA:\n",
    "        num_QA += 1\n",
    "        text = dataset[data_index]['question']\n",
    "        response = dataset[data_index]['response']\n",
    "   \n",
    "        data_index = data_index+1\n",
    "      \n",
    "        question_id = generate_unique_string()\n",
    "        context_id = generate_unique_string()\n",
    "        ctxs.append({\n",
    "                \"ID\":  context_id,\n",
    "                \"text\": text,\n",
    "                \"type\": 'mid',\n",
    "                \"Q_ID\": question_id,\n",
    "                \"GPT_answer\": response,\n",
    "            })\n",
    "        total_tokens += (word_len(text) + word_len(response))\n",
    "   \n",
    "        if total_tokens > 12000:\n",
    "            print(num_QA)\n",
    "            break\n",
    "    return data_index,total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(filepath):\n",
    "    \"\"\" Loads JSON data from a file \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from {filepath}: {e}\")\n",
    "        return []\n",
    "    \n",
    "def expand_and_enhance_data(data, data_index_1,data_index_2 ):\n",
    "    \"\"\" Expands each item by 32 times and enhances with ctxs \"\"\"\n",
    "    enhanced_data = []\n",
    "    for item in data:\n",
    "        for _ in range(1):  # Duplicate each item 32 times\n",
    "            # Select 8 other random items\n",
    "            other_items = random.sample(data, 29)\n",
    "            other_items.insert(0, item)  # Include the current item as the first item\n",
    "            new_item = item.copy()  # Make a copy of the item\n",
    "            new_item['ctxs'],  data_index_1,data_index_2 = add_ctxs_to_item(new_item, other_items, data_index_1,data_index_2)\n",
    "            enhanced_data.append(new_item)\n",
    "    random.shuffle(enhanced_data)  # Shuffle all items to mix them up\n",
    "    return enhanced_data, data_index_1,data_index_2\n",
    "\n",
    "def save_data(data, output_file):\n",
    "    \"\"\" Saves processed data into a JSON file \"\"\"\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# Main logic\n",
    "merged_file_path = 'merged_data.json'\n",
    "output_file = 'version_6_new_4k.json'\n",
    "merged_data = load_json_data(merged_file_path)\n",
    "merged_data = merged_data[20:4020]\n",
    "\n",
    "if merged_data:\n",
    "    data_index_1 = 0\n",
    "    data_index_2 = 0\n",
    "    enhanced_data,data_index_1,data_index_2 = expand_and_enhance_data(merged_data, data_index_1,data_index_2)\n",
    "    save_data(enhanced_data, output_file)\n",
    "    print(data_index_1,data_index_2)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to load data or context data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#变为行\n",
    "import json\n",
    "\n",
    "# 打开并读取JSON文件\n",
    "with open('version_6_4K_new.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 检查data是否是列表类型，如果是，按行写入每个元素\n",
    "if isinstance(data, list):\n",
    "    with open('Version_2.0/version6_no_orca_4K.json', 'w', encoding='utf-8') as output:\n",
    "        for item in data:\n",
    "            json.dump(item, output)\n",
    "            output.write('\\n')\n",
    "else:\n",
    "    print(\"JSON root is not a list. Cannot save items line by line.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version_10_end_version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_1 = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "\n",
    "dataset_2 = load_dataset(\"DKYoon/SlimPajama-6B\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载数据集\n",
    "dataset_1 = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "\n",
    "def filter_criteria(example):\n",
    "    # 计算单词数量\n",
    "    question_words = len(example['question'].split())\n",
    "    response_words = len(example['response'].split())\n",
    "    \n",
    "    # 将question转换为小写并检查是否含有单词\"translate\"或\"translation\"\n",
    "    question_lower = example['question'].lower()\n",
    "    contains_translate = \"translate\" in question_lower or \"translation\" in question_lower\n",
    "    \n",
    "    # 判断单词数量是否都小于1000且question中不含\"translate\"和\"translation\"\n",
    "    return question_words < 1000 and response_words < 1000 and not contains_translate\n",
    "\n",
    "# 应用过滤条件\n",
    "filtered_dataset = dataset_1.filter(filter_criteria)\n",
    "\n",
    "# 随机打乱数据集\n",
    "shuffled_dataset = filtered_dataset.shuffle()\n",
    "dataset_2 = dataset_2.shuffle()\n",
    "# 取出训练集\n",
    "filtered_dataset = shuffled_dataset['train']\n",
    "\n",
    "# 检查过滤后的数据\n",
    "print(filtered_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modify_gpt_answer_end(gpt_answer, correct_answer_id, question_index):\n",
    "    \"\"\" Modifies the GPT_answer to include the correct answer ID at the appropriate position \"\"\"\n",
    "    insert_text = f\"### Retrieval: The answer to question {question_index} can be found in ID {correct_answer_id}.\"\n",
    "    analysis_index = gpt_answer.find(\"### Analysis:\")\n",
    "    if analysis_index != -1:\n",
    "        part_before = gpt_answer[:analysis_index + len(\"### Analysis:\")]\n",
    "        part_after = gpt_answer[analysis_index + len(\"### Analysis:\"):]\n",
    "        return  insert_text + part_before  + part_after\n",
    "    else:\n",
    "        return  insert_text  + gpt_answer \n",
    "\n",
    "def generate_unique_string():\n",
    "    \"\"\" Generate a unique string based on hashing \"\"\"\n",
    "    unique_id = sha256(os.urandom(48)).hexdigest()[:24]\n",
    "    return unique_id\n",
    "\n",
    "import random\n",
    "\n",
    "def word_len(example):\n",
    "    # 计算单词数量\n",
    "    lens = len(example.split())\n",
    "    return lens\n",
    "  \n",
    "\n",
    "\n",
    "def add_ctxs_to_item(base_item, items,data_index_slimpajama,data_index_Orca):\n",
    "    \"\"\" Adds 'ctxs' key to a group of items and updates GPT_answer \"\"\"\n",
    "    ctxs = []\n",
    "    questions = []\n",
    "    response_ctxs = []\n",
    "    end_qa_pair= []\n",
    "    total_length = 0\n",
    "    answer_map = {}\n",
    "    # Process each item to create questions, answers, and modify GPT_answer\n",
    "    for idx, item in enumerate(items):\n",
    "        context_id = generate_unique_string()\n",
    "        question_id = generate_unique_string()\n",
    "\n",
    "        # middle answer \n",
    "        if idx < 5:\n",
    "            question_id = generate_unique_string()\n",
    "            ctxs.append({\n",
    "                \"ID\": context_id,\n",
    "                \"text\": item['context']+f\"### Question: \"+item['question'],\n",
    "                \"type\": 'mid',\n",
    "                \"Q_ID\": question_id,\n",
    "                \"GPT_answer\":item['GPT_answer'],\n",
    "            })\n",
    "            total_length += word_len(item['context']+f\"### Question: \"+item['question']) + word_len(item['GPT_answer'])\n",
    "        # last answer \n",
    "        elif idx < 10:\n",
    "            end_qa_pair.append(f\"\\nQuestion {idx-4}: {item['question']}\")\n",
    "            end_qa_pair.append(modify_gpt_answer_end(item['GPT_answer'], context_id, idx-4))\n",
    "            ctxs.append({\n",
    "                \"ID\": context_id,\n",
    "                \"text\": item['context'],\n",
    "                \"type\": 'end',\n",
    "                \"GPT_answer\":item['GPT_answer'],\n",
    "            })\n",
    "            total_length += word_len(item['context']+f\"### Question: \"+item['question']) + word_len(item['GPT_answer'])\n",
    "        else:\n",
    "            ctxs.append({\n",
    "                \"ID\": context_id,\n",
    "                \"text\": item['context'],\n",
    "                \"type\": 'false'\n",
    "            })\n",
    "            total_length += word_len(item['context']) \n",
    "    # Add random contexts from dataset\n",
    "    data_index_Orca,total_length= add_contexts_Orca(filtered_dataset,ctxs,data_index_Orca,total_length)\n",
    "    data_index_slimpajama,total_length= add_contexts(dataset_2, ctxs, data_index_slimpajama,total_length)\n",
    "    # Shuffle the list of contexts to ensure randomness\n",
    "    random.shuffle(ctxs)\n",
    "    # Reconstruct the mapping of question IDs to answer IDs after shuffling\n",
    "    for i, ctx in enumerate(ctxs):\n",
    "        if ctx.get('type') == 'mid':\n",
    "\n",
    "            insert_text = f\"Please answer the question ID: {ctx['ID']}, ### Answer: \"\n",
    "            response_id =generate_unique_string()\n",
    "            response_ctxs.append({\n",
    "                \"ID\": response_id,\n",
    "                \"text\": insert_text + ctx['GPT_answer'],\n",
    "                \"type\": 3 ,\n",
    "                \"location\" : i+1\n",
    "            })\n",
    "    offset = 0\n",
    "    for new_ctx in response_ctxs:\n",
    "        start_position = new_ctx['location'] + offset\n",
    "        # 检查起始位置是否超出列表长度，如果超出，则设置插入位置为列表末尾\n",
    "        if start_position >= len(ctxs):\n",
    "            insert_position = len(ctxs)\n",
    "        else:\n",
    "            insert_position = random.randint(start_position, len(ctxs))\n",
    "        ctxs.insert(insert_position, new_ctx)\n",
    "        offset += 1  \n",
    "\n",
    "    base_item['GPT_answer'] = '\\n '.join(end_qa_pair)\n",
    "    base_item['question'] = ' '.join(questions)\n",
    "    return ctxs, data_index_slimpajama,data_index_Orca\n",
    "\n",
    "\n",
    "def add_contexts(dataset, ctxs, data_index,total_length):\n",
    "    \"\"\" Adds contexts from the dataset while checking the token conditions \"\"\"\n",
    "    total_tokens = total_length\n",
    "    max_tokens=16000\n",
    "    while total_tokens < max_tokens:\n",
    "        text = dataset[data_index]['text']\n",
    "        data_index = data_index+1\n",
    "        word_count = len(text.split())\n",
    "        if 200 <= word_count <= 1000:\n",
    "            context_id = generate_unique_string()\n",
    "            ctxs.append({\"ID\": context_id, \"text\": text, \"type\": False})\n",
    "            total_tokens += word_count  # Update the token count assuming 1 token per word\n",
    "    return data_index,total_tokens\n",
    "\n",
    "\n",
    "def add_contexts_Orca(dataset, ctxs, data_index,total_length):\n",
    "    \"\"\" Adds contexts from the dataset while checking the token conditions \"\"\"\n",
    "    total_tokens = total_length\n",
    "    num_QA = 0\n",
    "    max_QA = 500\n",
    "    while num_QA  < max_QA:\n",
    "        num_QA += 1\n",
    "        text = dataset[data_index]['question']\n",
    "        \n",
    "        response = dataset[data_index]['response']\n",
    "        data_index = data_index+1\n",
    "        question_id = generate_unique_string()\n",
    "        \n",
    "        ctxs.append({\n",
    "                \"ID\":  question_id,\n",
    "                \"text\": text,\n",
    "                \"type\": 'mid',\n",
    "                \"Q_ID\": question_id,\n",
    "                \"GPT_answer\": response,\n",
    "            })\n",
    "        total_tokens += word_len(text) + len(response)\n",
    "        if total_tokens > 12000:\n",
    "            print(num_QA)\n",
    "            break\n",
    "    return data_index,total_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(filepath):\n",
    "    \"\"\" Loads JSON data from a file \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from {filepath}: {e}\")\n",
    "        return []\n",
    "    \n",
    "def expand_and_enhance_data(data,data_index_slimpajama,data_index_Orca):\n",
    "    \"\"\" Expands each item by 32 times and enhances with ctxs \"\"\"\n",
    "    enhanced_data = []\n",
    "    for item in data:\n",
    "        for _ in range(1):  # Duplicate each item 32 times\n",
    "            # Select 8 other random items\n",
    "            other_items = random.sample(data, 11)\n",
    "            other_items.insert(0, item)  # Include the current item as the first item\n",
    "            new_item = item.copy()  # Make a copy of the item\n",
    "            new_item['ctxs'], data_index_slimpajama,data_index_Orca = add_ctxs_to_item(new_item, other_items,data_index_slimpajama,data_index_Orca)\n",
    "            enhanced_data.append(new_item)\n",
    "    random.shuffle(enhanced_data)  # Shuffle all items to mix them up\n",
    "    print(data_index_slimpajama,data_index_Orca)\n",
    "    return enhanced_data\n",
    "\n",
    "import json\n",
    "\n",
    "def save_data(data, output_file):\n",
    "    \"\"\"Saves processed data into a JSON file, each list element on a new line if data is a list\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        with open(output_file, 'w', encoding='utf-8') as output:\n",
    "            for item in data:\n",
    "                # 转换每个元素为 JSON 字符串并写入文件，每个元素后加一个换行符\n",
    "                json.dump(item, output, ensure_ascii=False)\n",
    "                output.write('\\n')\n",
    "        print(f\"Data saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"JSON root is not a list. Cannot save items line by line.\")\n",
    "\n",
    "\n",
    "# Main logic\n",
    "merged_file_path = 'merged_data.json'\n",
    "output_file = 'version_11_test_4k.json'\n",
    "merged_data = load_json_data(merged_file_path)\n",
    "merged_data = merged_data[20:4020]\n",
    "\n",
    "if merged_data:\n",
    "    data_index_slimpajama = 0\n",
    "    data_index_Orca =0 \n",
    "    enhanced_data= expand_and_enhance_data(merged_data,data_index_slimpajama,data_index_Orca)\n",
    "    save_data(enhanced_data, output_file)\n",
    "   \n",
    "else:\n",
    "    print(\"Failed to load data or context data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version_12 End_version！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_1 = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "\n",
    "dataset_2 = load_dataset(\"DKYoon/SlimPajama-6B\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_criteria(example):\n",
    "    # 计算单词数量\n",
    "    question_words = len(example['question'].split())\n",
    "    response_words = len(example['response'].split())\n",
    "    \n",
    "    # 将question转换为小写并检查是否含有单词\"translate\"或\"translation\"\n",
    "    # question_lower = example['question'].lower()\n",
    "    # contains_translate = \"translate\" in question_lower or \"translation\" in question_lower\n",
    "    id_starts_with_cot = example['id'].startswith('cot')\n",
    "    # 判断单词数量是否都小于1000且question中不含\"translate\"和\"translation\"\n",
    "    # return 80 < question_words < 1000 and 80 < response_words < 1000 and not contains_translate and id_starts_with_cot\n",
    "    return  id_starts_with_cot and 20 < response_words and 20 < question_words\n",
    "\n",
    "# 应用过滤条件\n",
    "dataset_1 = dataset_1.filter(filter_criteria)\n",
    "\n",
    "# 随机打乱数据集\n",
    "dataset_1= dataset_1.shuffle()\n",
    "dataset_2 = dataset_2.shuffle()\n",
    "# 取出训练集\n",
    "filtered_dataset = dataset_1['train']\n",
    "dataset_1 = filtered_dataset\n",
    "# 检查过滤后的数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_unique_string():\n",
    "    \"\"\" Generate a unique string based on hashing \"\"\"\n",
    "    unique_id = sha256(os.urandom(48)).hexdigest()[:24]\n",
    "    return unique_id\n",
    "\n",
    "import random\n",
    "\n",
    "def word_len(example):\n",
    "    # 计算单词数量\n",
    "    lens = len(example.split())\n",
    "    return lens\n",
    "  \n",
    "\n",
    "\n",
    "def add_ctxs_to_item(base_item, items,data_index_slimpajama,data_index_Orca,length):\n",
    "    \"\"\" Adds 'ctxs' key to a group of items and updates GPT_answer \"\"\"\n",
    "    ctxs = []\n",
    "    questions = []\n",
    "    response_ctxs = []\n",
    "    end_qa_pair= []\n",
    "    total_length = 0\n",
    "    answer_map = {}\n",
    "    # Process each item to create questions, answers, and modify GPT_answer\n",
    "    for idx, item in enumerate(items):\n",
    "        question_id = generate_unique_string()\n",
    "        ctxs.append({\n",
    "                \"ID\": question_id,\n",
    "                \"text\": item['context']+f\" ### Question: \"+item['question'],\n",
    "                \"type\": 'question',\n",
    "                \"Q_ID\": question_id,\n",
    "                \"GPT_answer\":item['GPT_answer'],\n",
    "            })\n",
    "        total_length += word_len(item['context']+f\" ### Question: \"+item['question']+item['GPT_answer'])\n",
    "    data_index_Orca,total_length= add_contexts_Orca(dataset_1,ctxs,data_index_Orca,total_length,length)\n",
    "    data_index_slimpajama,total_length= add_contexts(dataset_2, ctxs, data_index_slimpajama,total_length,length)\n",
    "    # print(total_length)\n",
    "    random.shuffle(ctxs)\n",
    "    # Reconstruct the mapping of question IDs to answer IDs after shuffling\n",
    "    for i, ctx in enumerate(ctxs):\n",
    "        if ctx.get('type') == 'question':\n",
    "            questions.append(ctx['ID'])\n",
    "            end_qa_pair.append(f\"Answer question {ctx['ID']}: \")\n",
    "            end_qa_pair.append(ctx['GPT_answer'])\n",
    "    random.shuffle(ctxs)\n",
    "    base_item['GPT_answer'] = '\\n '.join(end_qa_pair)\n",
    "    base_item['question'] = ', '.join(questions)\n",
    "    return ctxs, data_index_slimpajama,data_index_Orca\n",
    "\n",
    "\n",
    "def add_contexts(dataset, ctxs, data_index,total_length,length):\n",
    "    \"\"\" Adds contexts from the dataset while checking the token conditions \"\"\"\n",
    "    total_tokens = total_length\n",
    "    max_tokens=length\n",
    "    while total_tokens < max_tokens:\n",
    "        text = dataset[data_index]['text']\n",
    "        data_index = data_index+1\n",
    "        word_count = len(text.split())\n",
    "        if 200 <= word_count <= 500:\n",
    "            context_id = generate_unique_string()\n",
    "            ctxs.append({\"ID\": context_id, \"text\": text, \"type\": False})\n",
    "            total_tokens += word_len(text)  # Update the token count assuming 1 token per word\n",
    "    return data_index,total_tokens\n",
    "\n",
    "\n",
    "def add_contexts_Orca(dataset, ctxs, data_index,total_length,length):\n",
    "    \"\"\" Adds contexts from the dataset while checking the token conditions \"\"\"\n",
    "    total_tokens = total_length\n",
    "    num_QA = 0\n",
    "    max_QA = 30\n",
    "    while num_QA  < max_QA:\n",
    "        num_QA += 1\n",
    "        text = dataset[data_index]['question']\n",
    "        response = dataset[data_index]['response']\n",
    "        data_index = data_index+1\n",
    "        question_id = generate_unique_string()\n",
    "        \n",
    "        ctxs.append({\n",
    "                \"ID\":  question_id,\n",
    "                \"text\": text,\n",
    "                \"type\": 'question',\n",
    "                \"Q_ID\": question_id,\n",
    "                \"GPT_answer\": response,\n",
    "            })\n",
    "        total_tokens += (word_len(text) + word_len(response))\n",
    "        if total_tokens > length:\n",
    "            print(num_QA)\n",
    "            break\n",
    "    return data_index,total_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(filepath):\n",
    "    \"\"\" Loads JSON data from a file \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from {filepath}: {e}\")\n",
    "        return []\n",
    "    \n",
    "def expand_and_enhance_data(data,data_index_slimpajama,data_index_Orca,length):\n",
    "    \"\"\" Expands each item by 32 times and enhances with ctxs \"\"\"\n",
    "    enhanced_data = []\n",
    "    for item in data:\n",
    "        for _ in range(1):  # Duplicate each item 32 times\n",
    "            # Select 8 other random items\n",
    "            other_items = random.sample(data, 20)\n",
    "            other_items.insert(0, item)  # Include the current item as the first item\n",
    "            new_item = item.copy()  # Make a copy of the item\n",
    "            new_item['ctxs'], data_index_slimpajama, data_index_Orca = add_ctxs_to_item(new_item, other_items,data_index_slimpajama,data_index_Orca,length)\n",
    "            enhanced_data.append(new_item)\n",
    "    random.shuffle(enhanced_data)  # Shuffle all items to mix them up\n",
    "    return enhanced_data,data_index_slimpajama,data_index_Orca\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def save_data(data, output_file, suffix):\n",
    "    \"\"\"Saves processed data into a JSON file, each list element on a new line if data is a list\"\"\"\n",
    "    # 确保输出目录存在，如果不存在则创建\n",
    "    os.makedirs(output_file, exist_ok=True)\n",
    "    \n",
    "    # 构建文件名，包括路径、基本文件名加上suffix后缀，以及文件扩展名\n",
    "    filename = f\"{output_file}data_{suffix}.json\"\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        with open(filename, 'w', encoding='utf-8') as output:\n",
    "            for item in data:\n",
    "                # 转换每个元素为 JSON 字符串并写入文件，每个元素后加一个换行符\n",
    "                json.dump(item, output, ensure_ascii=False)\n",
    "                output.write('\\n')\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    else:\n",
    "        print(\"JSON root is not a list. Cannot save items line by line.\")\n",
    "\n",
    "# Main logic\n",
    "merged_file_path = 'merged_data.json'\n",
    "output_file = 'version_12_test_4k/'\n",
    "merged_data = load_json_data(merged_file_path)\n",
    "merged_data = merged_data[20:4020]\n",
    "\n",
    "if merged_data:\n",
    "    data_index_slimpajama = 0\n",
    "    data_index_Orca =0 \n",
    "    \n",
    "    # 16 K\n",
    "    length = 20000\n",
    "    enhanced_data,data_index_slimpajama,data_index_Orca= expand_and_enhance_data(merged_data,data_index_slimpajama,data_index_Orca,length)\n",
    "    print(data_index_slimpajama,data_index_Orca)\n",
    "    save_data(enhanced_data, output_file,length)\n",
    "    # # 8 K\n",
    "    # length = length * 2\n",
    "    # enhanced_data,data_index_slimpajama,data_index_Orca= expand_and_enhance_data(merged_data,data_index_slimpajama,data_index_Orca,length)\n",
    "    # print(data_index_slimpajama,data_index_Orca)\n",
    "    # save_data(enhanced_data, output_file,length)\n",
    "    # # 16k\n",
    "    # length = length * 2 + 1100\n",
    "    # enhanced_data,data_index_slimpajama,data_index_Orca= expand_and_enhance_data(merged_data,data_index_slimpajama,data_index_Orca,length)\n",
    "    # print(data_index_slimpajama,data_index_Orca)\n",
    "    # save_data(enhanced_data, output_file,length)\n",
    "    # # 32K\n",
    "    # length = length * 2\n",
    "    # enhanced_data = expand_and_enhance_data(merged_data,data_index_slimpajama,data_index_Orca,length)\n",
    "    # save_data(enhanced_data, output_file,length)\n",
    "\n",
    "\n",
    "   \n",
    "else:\n",
    "    print(\"Failed to load data or context data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "265286 orcca orca 384W ->40K?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def tokenize_data(text_id):\n",
    "    \"\"\"Tokenize text data and generate ID.\"\"\"\n",
    "    text, tokenizer = text_id\n",
    "    tokenized_text = tokenizer(text, truncation=True, max_length=1000)\n",
    "    # Convert BatchEncoding to dictionary and extract necessary fields\n",
    "    tokenized_text_data = {\n",
    "        'input_ids': tokenized_text['input_ids'],\n",
    "\n",
    "    }\n",
    "    return tokenized_text_data\n",
    "\n",
    "def filter_by_word_count(example):\n",
    "    \"\"\"Calculate the number of words in the text.\"\"\"\n",
    "    word_count = len(example['text'].split())\n",
    "    return 50 < word_count < 800\n",
    "\n",
    "def main():\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"DKYoon/SlimPajama-6B\", split='train')\n",
    "    # Take the first 1 million rows that meet the word count criteria\n",
    "    num = 1000000\n",
    "    dataset = dataset.select(range(num)).filter(filter_by_word_count)\n",
    "    print(len(dataset))\n",
    "    # Load tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "    # Prepare data for multiprocessing\n",
    "    texts = [(text, tokenizer) for text in dataset['text']]  # Assuming 'text' field exists\n",
    "\n",
    "    # Use multiprocessing to tokenize data\n",
    "    with Pool(os.cpu_count()) as pool:\n",
    "        results = pool.map(tokenize_data, texts)\n",
    "\n",
    "    # Convert results to DataFrame and save as Parquet file\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_parquet('tokenized_data_SlimPajama_Top1M.parquet')\n",
    "\n",
    "    print(\"Data tokenized and saved successfully as Parquet.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def filter_criteria(example):\n",
    "    # 计算单词数量\n",
    "    question_words = len(example['question'].split())\n",
    "    response_words = len(example['response'].split())\n",
    "    # 检查 id 是否以 'cot' 开头\n",
    "    question_lower = example['question'].lower()\n",
    "    contains_translate = \"translate\" in question_lower or \"translation\" in question_lower\n",
    "    \n",
    "    # 判断单词数量是否都小于1000且question中不含\"translate\"和\"translation\"\n",
    "    return 80 < question_words < 800 and 100 < response_words < 800 and not contains_translate\n",
    "\n",
    "\n",
    "def tokenize_data(example, tokenizer):\n",
    "    \"\"\"Tokenize question and response data.\"\"\"\n",
    "    tokenized_question = tokenizer(example['question'])\n",
    "    tokenized_response = tokenizer(example['response'])\n",
    "    return {\n",
    "        'question_input_ids': tokenized_question['input_ids'],\n",
    "        'response_input_ids': tokenized_response['input_ids']\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"Open-Orca/OpenOrca\", split='train')\n",
    "    # Filter dataset based on criteria\n",
    "    filtered_dataset = dataset.filter(filter_criteria)\n",
    "    print(filtered_dataset)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "    # Prepare data for multiprocessing\n",
    "    pool = Pool(os.cpu_count())\n",
    "    results = [pool.apply_async(tokenize_data, (example, tokenizer)) for example in filtered_dataset]\n",
    "    results = [result.get() for result in results]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Convert results to DataFrame and save as Parquet file\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_parquet('tokenized_data_Orca_all.parquet')\n",
    "\n",
    "    print(\"Data tokenized and saved successfully as Parquet.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# token dataset version 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 设置文件路径\n",
    "file_path_1 = \"tokenized_data_Orca_all.parquet\"\n",
    "file_path_2 = \"tokenized_data_SlimPajama_Top1M.parquet\"\n",
    "# 加载数据集\n",
    "dataset_Orca = load_dataset('parquet', data_files=file_path_1)\n",
    "dataset_Slim = load_dataset('parquet', data_files=file_path_2)\n",
    "# 查看数据集\n",
    "dataset_Orca = dataset_Orca['train']\n",
    "dataset_Slim = dataset_Slim['train']\n",
    "dataset_Slim = dataset_Slim.shuffle()\n",
    "dataset_Orca = dataset_Orca.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 751834\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_Slim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question_input_ids', 'response_input_ids'],\n",
       "    num_rows: 421456\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_Orca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_Slim[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_unique_string():\n",
    "    \"\"\" Generate a unique string based on hashing \"\"\"\n",
    "    unique_id = sha256(os.urandom(48)).hexdigest()[:24]\n",
    "    return unique_id\n",
    "\n",
    "import random\n",
    "\n",
    "def word_len(example):\n",
    "    # 计算单词数量\n",
    "    lens = len(example.split())\n",
    "    return lens\n",
    "  \n",
    "def token(text,tokenizer):\n",
    "    \"\"\"Tokenize text data and generate ID.\"\"\"\n",
    "   \n",
    "    tokenized_text = tokenizer(text, truncation=True, max_length=10000)\n",
    "    \n",
    "    return tokenized_text['input_ids']\n",
    "\n",
    "def input_start(input,label,length,tokenizer):\n",
    "    context = f\"\\n### Instruction: Please carefully read the text and answer the questions contained within.\\n\"\n",
    "    context += f\"### Context:\\n\"\n",
    "    add_list = token(context,tokenizer)\n",
    "    # print(type(add_list))\n",
    "    # print(type(input))\n",
    "    input = input + add_list\n",
    "    label = label +  [-100] * len(add_list)\n",
    "    length = length + len(add_list)\n",
    "    return  input, label, length \n",
    "\n",
    "def add_ctxs_to_item(items,data_index_slimpajama,data_index_Orca,length):\n",
    "    \"\"\" Adds 'ctxs' key to a group of items and updates GPT_answer \"\"\"\n",
    "    ctxs = []\n",
    "\n",
    "    questions_ctxs =[]\n",
    "  \n",
    "    total_length = 0\n",
    "    Slim_length = []\n",
    "    Orca_Q_length = []\n",
    "    Orca_A_length = []\n",
    "    Squad_Q_length = []\n",
    "    Squad_A_length = []\n",
    "    input_ids =[]\n",
    "    labels = []\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    input_ids,labels,total_length = input_start(input_ids,labels,total_length,tokenizer)\n",
    "\n",
    "    # Process each item to create questions, answers, and modify GPT_answer\n",
    "    for idx, item in enumerate(items):\n",
    "        # mid\n",
    "        if idx < 6:\n",
    "            context_id = generate_unique_string()\n",
    "            question_id = generate_unique_string()\n",
    "            c_id = token(f\"\\nID: {context_id}, \",tokenizer)\n",
    "            q_id = token(f\"\\nID: {question_id}, Answer question {context_id}: \",tokenizer)\n",
    "            text = token(item['context']+f\" ### Question: \"+item['question'],tokenizer)\n",
    "            GPT_answer = token(item['GPT_answer'],tokenizer)\n",
    "            ctxs.append({\n",
    "                    \"ID\": c_id+text,\n",
    "                     #\"text\": text,\n",
    "                    \"type\": 'question',\n",
    "                    \"Q_ID\": q_id,\n",
    "                    \"GPT_answer\":GPT_answer,\n",
    "                })\n",
    "            total_length += len(q_id+c_id+text+GPT_answer)\n",
    "            Squad_Q_length += [len(text)]\n",
    "            Squad_A_length += [len(GPT_answer)]\n",
    "        #end \n",
    "        elif idx < 12:\n",
    "            context_id = generate_unique_string()\n",
    "            question_id = generate_unique_string()\n",
    "            c_id = token(f\"\\nID: {context_id}, \",tokenizer)\n",
    "            q_id = token(f\"\\nQuestion {idx-5}: {item['question']}\\n Retrieval: The answer to question {idx-5} can be found in ID {context_id}.\", tokenizer)\n",
    "            text = token(item['context']+f\" ### Question: \"+item['question'],tokenizer)\n",
    "            GPT_answer = token(item['GPT_answer'],tokenizer)\n",
    "            ctxs.append({\n",
    "                    \"ID\": c_id+text,\n",
    "                    # \"text\": text,\n",
    "                    \"type\": 'end',\n",
    "                    \"Q_ID\": q_id+GPT_answer,\n",
    "                    #\"GPT_answer\":GPT_answer,\n",
    "                })\n",
    "            total_length += len(q_id+c_id+text+GPT_answer)\n",
    "            Squad_Q_length += [len(text)]\n",
    "            Squad_A_length += [len(GPT_answer)]\n",
    "        else:\n",
    "            context_id = generate_unique_string()\n",
    "            c_id = token(f\"\\nID: {context_id}, \",tokenizer)\n",
    "            text = token(item['context']+f\" ### Question: \"+item['question'],tokenizer)\n",
    "            ctxs.append({\n",
    "                    \"ID\": c_id+text,\n",
    "                    \"type\": False,\n",
    "                })\n",
    "            total_length += len(c_id+text)\n",
    "            Squad_Q_length += [len(text)]\n",
    "\n",
    "    data_index_Orca,total_length= add_contexts_Orca(dataset_Orca,ctxs,data_index_Orca,total_length,tokenizer,Orca_Q_length,Orca_A_length)\n",
    "\n",
    "\n",
    "    ## context end:\n",
    "    Instruction = token(f\"\\n Provide a detailed analysis based on the Context and offer your answers. Question: \",tokenizer)\n",
    "    total_length += len(Instruction)    \n",
    "\n",
    "    data_index_slimpajama,total_length= add_contexts(dataset_Slim, ctxs, data_index_slimpajama,total_length,length,tokenizer)\n",
    "\n",
    "\n",
    "    random.shuffle(ctxs)\n",
    "    # Reconstruct the mapping of question IDs to answer IDs after shuffling\n",
    " \n",
    "    for i, ctx in enumerate(ctxs):\n",
    "        if ctx.get('type') == 'question':\n",
    "            questions_ctxs.append({\n",
    "                \"ID\": ctx['Q_ID'],\n",
    "                \"text\": ctx['GPT_answer'],\n",
    "                \"type\": 'Answer',\n",
    "                \"location\" : i+1\n",
    "            })\n",
    "\n",
    "    offset = 0\n",
    "    random.shuffle(questions_ctxs)\n",
    "    for new_ctx in questions_ctxs:\n",
    "        start_position = new_ctx['location'] + offset\n",
    "        # 检查起始位置是否超出列表长度，如果超出，则设置插入位置为列表末尾\n",
    "        if start_position >= len(ctxs):\n",
    "            insert_position = len(ctxs)\n",
    "        else:\n",
    "            insert_position = random.randint(start_position, len(ctxs))\n",
    "        ctxs.insert(insert_position, new_ctx)\n",
    "        offset += 1  \n",
    "\n",
    "    # get end_data\n",
    "    for item in ctxs:\n",
    "        if item.get('type') == 'Answer':\n",
    "            input_ids += item['ID']\n",
    "            labels += [-100] * len(item['ID'])\n",
    "            input_ids += item['text']\n",
    "            labels += item['text']\n",
    "        if item.get('type') == False or item.get('type') == 'question' or item.get('type') == 'end':\n",
    "            input_ids += item['ID']\n",
    "            labels += [-100] * len(item['ID'])\n",
    "    \n",
    "    # print(\"==\"*100)\n",
    "    # print('input len:',len(input_ids))\n",
    "    # print('labels len:',len(labels))\n",
    "    # print('length: ',total_length)\n",
    "\n",
    "    input_ids += Instruction \n",
    "    labels += [-100] * len(Instruction)\n",
    "  \n",
    "    \n",
    "    for item in ctxs:\n",
    "        if item.get('type') == 'end':\n",
    "            input_ids += item['Q_ID']\n",
    "            labels += item['Q_ID']\n",
    "\n",
    "    # print(\"==\"*100)\n",
    "    # print('input len:',len(input_ids))\n",
    "    # print('labels len:',len(labels))\n",
    "    # print('length: ',total_length)\n",
    "    return {'input_ids':input_ids,'labels':labels}, {'Orca_Q':Orca_Q_length,'Orca_A':Orca_A_length,'Squad_Q': Squad_Q_length,'Squad_A':Squad_A_length},data_index_slimpajama,data_index_Orca\n",
    "\n",
    "\n",
    "def add_contexts(dataset, ctxs, data_index,total_length,length,tokenizer):\n",
    "    \"\"\" Adds contexts from the dataset while checking the token conditions \"\"\"\n",
    "    total_tokens = total_length\n",
    "    max_tokens= length\n",
    "    while total_tokens < max_tokens:\n",
    "        text = dataset[data_index]['input_ids']\n",
    "        data_index = data_index+1\n",
    "        context_id = generate_unique_string()\n",
    "        c_id = token(f\"\\nID: {context_id}, \",tokenizer)\n",
    "        total_tokens += len(c_id+text)\n",
    "        if (total_tokens <= max_tokens):\n",
    "            ctxs.append({\"ID\": c_id+text,\"type\": False})\n",
    "        else:\n",
    "            all = c_id + text\n",
    "            ctxs.append({\"ID\": all[:max_tokens-total_tokens], \"type\": False})\n",
    "            total_tokens = 32768\n",
    "          # Update the token count assuming 1 token per word\n",
    "    return data_index,total_tokens\n",
    "\n",
    "\n",
    "def add_contexts_Orca(dataset, ctxs, data_index,total_length,tokenizer,Orca_Q_length,Orca_A_length\n",
    "    ):\n",
    "    \"\"\" Adds contexts from the dataset while checking the token conditions \"\"\"\n",
    "\n",
    "    # features: ['question_input_ids', 'response_input_ids'],\n",
    "    total_tokens = total_length\n",
    "    num_QA = 0\n",
    "    max_QA = 80\n",
    "    while num_QA  < max_QA:\n",
    "        num_QA += 1\n",
    "        context_id = generate_unique_string()\n",
    "        question_id = generate_unique_string()\n",
    "        text = dataset[data_index]['question_input_ids']\n",
    "        response = dataset[data_index]['response_input_ids']\n",
    "        data_index += 1\n",
    "        c_id = token(f\"\\nID: {context_id}, \",tokenizer)\n",
    "        q_id = token(f\"\\nID: {question_id}, \",tokenizer)  \n",
    "        q_start = token(f\" Answer question {context_id}: \",tokenizer)  \n",
    "        ctxs.append({\n",
    "                \"ID\":  c_id+text,\n",
    "                # \"text\": text,\n",
    "                \"type\": 'question',\n",
    "                \"Q_ID\": q_id+q_start,\n",
    "                \"GPT_answer\":response,\n",
    "            })\n",
    "        total_tokens += len(c_id+text+q_id+q_start+response)\n",
    "        Orca_Q_length += [len(text)]\n",
    "        Orca_A_length += [len(response)]\n",
    "\n",
    "        if total_tokens > 24000:\n",
    "            # print(num_QA)\n",
    "            break\n",
    "    return data_index,total_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376 1973\n",
      "Data saved to token_version_6_32K_version_5.parquet\n",
      "Data saved to distribution.parquet\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def load_json_data(filepath):\n",
    "    \"\"\" Loads JSON data from a file \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from {filepath}: {e}\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "def expand_and_enhance_data(data,data_index_slimpajama,data_index_Orca,length):\n",
    "    \"\"\" Expands each item by 32 times and enhances with ctxs \"\"\"\n",
    "    enhanced_data = []\n",
    "    length_distri = []\n",
    "    for item in data:\n",
    "        for _ in range(4):  # Duplicate each item 32 times\n",
    "            # Select 8 other random items\n",
    "            other_items = random.sample(data, 14)\n",
    "            other_items.insert(0, item)  # Include the current item as the first item\n",
    "            new_item = item.copy()  # Make a copy of the item\n",
    "            end_data, length_distribution, data_index_slimpajama, data_index_Orca = add_ctxs_to_item(other_items,data_index_slimpajama,data_index_Orca,length)\n",
    "            enhanced_data.append(end_data)\n",
    "            length_distri.append(length_distribution)\n",
    "    random.shuffle(enhanced_data)  # Shuffle all items to mix them up\n",
    "    return enhanced_data,length_distri,data_index_slimpajama,data_index_Orca\n",
    "\n",
    "\n",
    "\n",
    "def save_data_1(data, output_file):\n",
    "    \"\"\" Save the data into a parquet file \"\"\"\n",
    "    df = pd.DataFrame(data, columns=['input_ids', 'labels'])\n",
    "    df.to_parquet(output_file, index=False)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "def save_data_2(data, output_file):\n",
    "    \"\"\" Save the data into a parquet file \"\"\"\n",
    "    df = pd.DataFrame(data, columns=['Orca_Q','Orca_A','Squad_Q','Squad_A'])\n",
    "    df.to_parquet(output_file, index=False)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "\n",
    "\n",
    "# Main logic\n",
    "merged_file_path = 'merged_data.json'\n",
    "output_file = 'token_version_6_32K_version_5.parquet'\n",
    "output_file_2 = 'distribution.parquet'\n",
    "merged_data = load_json_data(merged_file_path)\n",
    "merged_data = merged_data[20:4020]\n",
    "\n",
    "if merged_data:\n",
    "    data_index_slimpajama = 0\n",
    "    data_index_Orca =0 \n",
    "    \n",
    "    # 16 K\n",
    "    length = 32768\n",
    "    enhanced_data,length_distri,data_index_slimpajama,data_index_Orca= expand_and_enhance_data(merged_data,data_index_slimpajama,data_index_Orca,length)\n",
    "    print(data_index_slimpajama,data_index_Orca)\n",
    "    save_data_1(enhanced_data, output_file)\n",
    "    save_data_2(length_distri, output_file_2)\n",
    "else:\n",
    "    print(\"Failed to load data or context data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 80 examples [00:00, 31530.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "file_path_1 = \"distribution.parquet\"\n",
    "\n",
    "# 加载数据集\n",
    "dataset= load_dataset('parquet', data_files=file_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Orca_Q': [286,\n",
       "  199,\n",
       "  406,\n",
       "  211,\n",
       "  315,\n",
       "  198,\n",
       "  205,\n",
       "  801,\n",
       "  630,\n",
       "  240,\n",
       "  459,\n",
       "  239,\n",
       "  134,\n",
       "  207,\n",
       "  260,\n",
       "  308,\n",
       "  440,\n",
       "  553,\n",
       "  209,\n",
       "  222,\n",
       "  288,\n",
       "  124,\n",
       "  1000,\n",
       "  1000,\n",
       "  183,\n",
       "  474,\n",
       "  224,\n",
       "  612,\n",
       "  553],\n",
       " 'Orca_A': [180,\n",
       "  140,\n",
       "  144,\n",
       "  142,\n",
       "  198,\n",
       "  216,\n",
       "  177,\n",
       "  199,\n",
       "  213,\n",
       "  234,\n",
       "  161,\n",
       "  124,\n",
       "  263,\n",
       "  171,\n",
       "  288,\n",
       "  145,\n",
       "  204,\n",
       "  131,\n",
       "  159,\n",
       "  294,\n",
       "  224,\n",
       "  316,\n",
       "  176,\n",
       "  324,\n",
       "  206,\n",
       "  178,\n",
       "  315,\n",
       "  155,\n",
       "  192],\n",
       " 'Squad_Q': [190,\n",
       "  153,\n",
       "  244,\n",
       "  265,\n",
       "  115,\n",
       "  250,\n",
       "  169,\n",
       "  182,\n",
       "  247,\n",
       "  281,\n",
       "  190,\n",
       "  192,\n",
       "  277,\n",
       "  257,\n",
       "  252],\n",
       " 'Squad_A': [210, 74, 190, 208, 245, 196, 119, 241, 154, 186]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4000):\n",
    "    input_len = len(dataset['train'][i]['input_ids'])\n",
    "    label_len = len(dataset['train'][i]['labels'])\n",
    "    if input_len != 32768:\n",
    "        print(\"len:\",input_len)\n",
    "        print(i)\n",
    "    if label_len != 32768:\n",
    "        print(\"len:\",label_len)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "file_path_1 = \"/home/yuhao/gsm8k-ScRel/data_Context/context_data/tokenized_data_Orca_all.parquet\"\n",
    "\n",
    "# 加载数据集\n",
    "dataset= load_dataset('parquet', data_files=file_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']\n",
    "concatenated_ids = [q + r for q, r in zip(dataset['question_input_ids'], dataset['response_input_ids'])]\n",
    "\n",
    "# Flatten the list of IDs and then concatenate multiple entries together\n",
    "all_input_ids = list(chain(*concatenated_ids))\n",
    "\n",
    "# Split the long sequence into chunks of 32768 tokens each\n",
    "chunks = [all_input_ids[i:i + 32769] for i in range(0, len(all_input_ids), 32769)]\n",
    "\n",
    "# If the last chunk is not full, you can choose to discard it\n",
    "if len(chunks[-1]) != 32769:\n",
    "    chunks.pop()  # This removes the last incomplete chunk\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'input_ids': chunks})\n",
    "\n",
    "# Save to a Parquet file\n",
    "df.to_parquet('short_Orca.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 8168 examples [00:03, 2154.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 8168\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_1 = \"short_Orca.parquet\"\n",
    "\n",
    "# 加载数据集\n",
    "dataset= load_dataset('parquet', data_files=file_path_1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "all_input_ids = list(chain(*dataset['input_ids']))\n",
    "\n",
    "# 按每 32769 个元素分割长序列\n",
    "chunks = [all_input_ids[i:i + 32769] for i in range(0, len(all_input_ids), 32769)]\n",
    "\n",
    "# 如果最后一个 chunk 不满 32769，可以选择丢弃或处理它\n",
    "if len(chunks[-1]) != 32769:\n",
    "    chunks.pop()  # 如果你不想保留最后一个不完整的 chunk\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame({'input_ids': chunks})\n",
    "\n",
    "# 保存到 Parquet 文件\n",
    "df.to_parquet('short_slim.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/anaconda/envs/FILM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "file_path_1 = \"/home/yuhao/gsm8k-ScRel/data_Context/context_data/tokenized_data_Orca_all.parquet\"\n",
    "\n",
    "# 加载数据集\n",
    "dataset= load_dataset('parquet', data_files=file_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_input_ids', 'response_input_ids'],\n",
       "        num_rows: 421456\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 32769\n",
      "0\n",
      "len: 32769\n",
      "1\n",
      "len: 32769\n",
      "2\n",
      "len: 32769\n",
      "3\n",
      "len: 32769\n",
      "4\n",
      "len: 32769\n",
      "5\n",
      "len: 32769\n",
      "6\n",
      "len: 32769\n",
      "7\n",
      "len: 32769\n",
      "8\n",
      "len: 32769\n",
      "9\n",
      "len: 32769\n",
      "10\n",
      "len: 32769\n",
      "11\n",
      "len: 32769\n",
      "12\n",
      "len: 32769\n",
      "13\n",
      "len: 32769\n",
      "14\n",
      "len: 32769\n",
      "15\n",
      "len: 32769\n",
      "16\n",
      "len: 32769\n",
      "17\n",
      "len: 32769\n",
      "18\n",
      "len: 32769\n",
      "19\n",
      "len: 32769\n",
      "20\n",
      "len: 32769\n",
      "21\n",
      "len: 32769\n",
      "22\n",
      "len: 32769\n",
      "23\n",
      "len: 32769\n",
      "24\n",
      "len: 32769\n",
      "25\n",
      "len: 32769\n",
      "26\n",
      "len: 32769\n",
      "27\n",
      "len: 32769\n",
      "28\n",
      "len: 32769\n",
      "29\n",
      "len: 32769\n",
      "30\n",
      "len: 32769\n",
      "31\n",
      "len: 32769\n",
      "32\n",
      "len: 32769\n",
      "33\n",
      "len: 32769\n",
      "34\n",
      "len: 32769\n",
      "35\n",
      "len: 32769\n",
      "36\n",
      "len: 32769\n",
      "37\n",
      "len: 32769\n",
      "38\n",
      "len: 32769\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    input_len = len(dataset['train'][i]['input_ids'])\n",
    "    #label_len = len(dataset['train'][i]['labels'])\n",
    "    if input_len != 32768:\n",
    "        print(\"len:\",input_len)\n",
    "        print(i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
