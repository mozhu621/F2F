{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenized_data_SlimPajama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 40/40 [00:00<00:00, 6704.99 examples/s]\n",
      "/home/yuhao/anaconda/envs/easycontext/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "Data tokenized and saved successfully as Parquet.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def tokenize_data(text_id):\n",
    "    \"\"\"Tokenize text data and generate ID.\"\"\"\n",
    "    text, tokenizer = text_id\n",
    "    tokenized_text = tokenizer(text, truncation=True, max_length=1000)\n",
    "    # Convert BatchEncoding to dictionary and extract necessary fields\n",
    "    tokenized_text_data = {\n",
    "        'input_ids': tokenized_text['input_ids'],\n",
    "\n",
    "    }\n",
    "    return tokenized_text_data\n",
    "\n",
    "def filter_by_word_count(example):\n",
    "    \"\"\"Calculate the number of words in the text.\"\"\"\n",
    "    word_count = len(example['text'].split())\n",
    "    return 50 < word_count < 800\n",
    "\n",
    "def main(data_row):\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"DKYoon/SlimPajama-6B\", split='train')\n",
    "    # Take the first 1 million rows that meet the word count criteria\n",
    "    # num = 5000000\n",
    "    num = data_row\n",
    "    dataset = dataset.select(range(num)).filter(filter_by_word_count)\n",
    "    print(len(dataset))\n",
    "    # Load tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "    # Prepare data for multiprocessing\n",
    "    texts = [(text, tokenizer) for text in dataset['text']]  # Assuming 'text' field exists\n",
    "\n",
    "    # Use multiprocessing to tokenize data\n",
    "    with Pool(os.cpu_count()) as pool:\n",
    "        results = pool.map(tokenize_data, texts)\n",
    "\n",
    "    # Convert results to DataFrame and save as Parquet file\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_parquet('tokenized_data_SlimPajama.parquet')\n",
    "\n",
    "    print(\"Data tokenized and saved successfully as Parquet.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main(5000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenized_data_Orca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 12.0k/12.0k [00:00<00:00, 20.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def filter_criteria(example):\n",
    "    # 计算单词数量\n",
    "    question_words = len(example['question'].split())\n",
    "    response_words = len(example['response'].split())\n",
    "    # 检查 id 是否以 'cot' 开头\n",
    "    question_lower = example['question'].lower()\n",
    "    contains_translate = \"translate\" in question_lower or \"translation\" in question_lower\n",
    "    \n",
    "    # 判断单词数量是否都小于1000且question中不含\"translate\"和\"translation\"\n",
    "    return 80 < question_words < 600 and 80 < response_words < 600 and not contains_translate\n",
    "\n",
    "\n",
    "def tokenize_data(example, tokenizer):\n",
    "    \"\"\"Tokenize question and response data.\"\"\"\n",
    "    tokenized_question = tokenizer(example['question'])\n",
    "    tokenized_response = tokenizer(example['response'])\n",
    "    return {\n",
    "        'question_input_ids': tokenized_question['input_ids'],\n",
    "        'response_input_ids': tokenized_response['input_ids']\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"Open-Orca/OpenOrca\", split='train')\n",
    "    # Filter dataset based on criteria\n",
    "    filtered_dataset = dataset.filter(filter_criteria)\n",
    "    print(filtered_dataset)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "    # Prepare data for multiprocessing\n",
    "    pool = Pool(os.cpu_count())\n",
    "    results = [pool.apply_async(tokenize_data, (example, tokenizer)) for example in filtered_dataset]\n",
    "    results = [result.get() for result in results]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Convert results to DataFrame and save as Parquet file\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_parquet('tokenized_data_Orca_all.parquet')\n",
    "\n",
    "    print(\"Data tokenized and saved successfully as Parquet.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'meta', '__index_level_0__'],\n",
       "    num_rows: 5489000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easycontext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
