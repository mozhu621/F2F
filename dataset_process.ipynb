{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenized_data_SlimPajama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 40/40 [00:00<00:00, 6704.99 examples/s]\n",
      "/home/yuhao/anaconda/envs/easycontext/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "Data tokenized and saved successfully as Parquet.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def tokenize_data(text_id):\n",
    "    \"\"\"Tokenize text data and generate ID.\"\"\"\n",
    "    text, tokenizer = text_id\n",
    "    tokenized_text = tokenizer(text, truncation=True, max_length=1000)\n",
    "    # Convert BatchEncoding to dictionary and extract necessary fields\n",
    "    tokenized_text_data = {\n",
    "        'input_ids': tokenized_text['input_ids'],\n",
    "\n",
    "    }\n",
    "    return tokenized_text_data\n",
    "\n",
    "def filter_by_word_count(example):\n",
    "    \"\"\"Calculate the number of words in the text.\"\"\"\n",
    "    word_count = len(example['text'].split())\n",
    "    return 50 < word_count < 800\n",
    "\n",
    "def main(data_row):\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"DKYoon/SlimPajama-6B\", split='train')\n",
    "    # Take the first 1 million rows that meet the word count criteria\n",
    "    # num = 5000000\n",
    "    num = data_row\n",
    "    dataset = dataset.select(range(num)).filter(filter_by_word_count)\n",
    "    print(len(dataset))\n",
    "    # Load tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "    # Prepare data for multiprocessing\n",
    "    texts = [(text, tokenizer) for text in dataset['text']]  # Assuming 'text' field exists\n",
    "\n",
    "    # Use multiprocessing to tokenize data\n",
    "    with Pool(os.cpu_count()) as pool:\n",
    "        results = pool.map(tokenize_data, texts)\n",
    "\n",
    "    # Convert results to DataFrame and save as Parquet file\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_parquet('tokenized_data_SlimPajama.parquet')\n",
    "\n",
    "    print(\"Data tokenized and saved successfully as Parquet.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main(5000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenized_data_Orca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 12.0k/12.0k [00:00<00:00, 20.9MB/s]\n",
      "Filter: 100%|██████████| 4233923/4233923 [01:09<00:00, 61195.89 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'system_prompt', 'question', 'response'],\n",
      "    num_rows: 534723\n",
      "})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData tokenized and saved successfully as Parquet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m pool \u001b[38;5;241m=\u001b[39m Pool(os\u001b[38;5;241m.\u001b[39mcpu_count())\n\u001b[1;32m     41\u001b[0m results \u001b[38;5;241m=\u001b[39m [pool\u001b[38;5;241m.\u001b[39mapply_async(tokenize_data, (example, tokenizer)) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m filtered_dataset]\n\u001b[0;32m---> 42\u001b[0m results \u001b[38;5;241m=\u001b[39m [result\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m     43\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     44\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m pool \u001b[38;5;241m=\u001b[39m Pool(os\u001b[38;5;241m.\u001b[39mcpu_count())\n\u001b[1;32m     41\u001b[0m results \u001b[38;5;241m=\u001b[39m [pool\u001b[38;5;241m.\u001b[39mapply_async(tokenize_data, (example, tokenizer)) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m filtered_dataset]\n\u001b[0;32m---> 42\u001b[0m results \u001b[38;5;241m=\u001b[39m [\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m     43\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     44\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/anaconda/envs/easycontext/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/easycontext/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/easycontext/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda/envs/easycontext/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from hashlib import sha256\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def filter_criteria(example):\n",
    "    # 计算单词数量\n",
    "    question_words = len(example['question'].split())\n",
    "    response_words = len(example['response'].split())\n",
    "    # 检查 id 是否以 'cot' 开头\n",
    "    question_lower = example['question'].lower()\n",
    "    contains_translate = \"translate\" in question_lower or \"translation\" in question_lower\n",
    "    \n",
    "    # 判断单词数量是否都小于1000且question中不含\"translate\"和\"translation\"\n",
    "    return 80 < question_words < 600 and 80 < response_words < 600 and not contains_translate\n",
    "\n",
    "\n",
    "def tokenize_data(example, tokenizer):\n",
    "    \"\"\"Tokenize question and response data.\"\"\"\n",
    "    tokenized_question = tokenizer(example['question'])\n",
    "    tokenized_response = tokenizer(example['response'])\n",
    "    return {\n",
    "        'question_input_ids': tokenized_question['input_ids'],\n",
    "        'response_input_ids': tokenized_response['input_ids']\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"Open-Orca/OpenOrca\", split='train')\n",
    "    # Filter dataset based on criteria\n",
    "    filtered_dataset = dataset.filter(filter_criteria)\n",
    "    print(filtered_dataset)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "    # Prepare data for multiprocessing\n",
    "    pool = Pool(os.cpu_count())\n",
    "    results = [pool.apply_async(tokenize_data, (example, tokenizer)) for example in filtered_dataset]\n",
    "    results = [result.get() for result in results]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Convert results to DataFrame and save as Parquet file\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_parquet('tokenized_data_Orca_all.parquet')\n",
    "\n",
    "    print(\"Data tokenized and saved successfully as Parquet.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'meta', '__index_level_0__'],\n",
       "    num_rows: 5489000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easycontext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
