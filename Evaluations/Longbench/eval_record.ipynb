{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from metrics import (\n",
    "    qa_f1_score,\n",
    "    rouge_zh_score,\n",
    "    qa_f1_zh_score,\n",
    "    rouge_score,\n",
    "    classification_score,\n",
    "    retrieval_score,\n",
    "    retrieval_zh_score,\n",
    "    count_score,\n",
    "    code_sim_score,\n",
    ")\n",
    "\n",
    "\n",
    "dataset2metric = {\n",
    "    \"narrativeqa\": qa_f1_score,\n",
    "    \"qasper\": qa_f1_score,\n",
    "    \"multifieldqa_en\": qa_f1_score,\n",
    "    \"multifieldqa_zh\": qa_f1_zh_score,\n",
    "    \"hotpotqa\": qa_f1_score,\n",
    "    \"2wikimqa\": qa_f1_score,\n",
    "    \"musique\": qa_f1_score,\n",
    "    \"dureader\": rouge_zh_score,\n",
    "    \"gov_report\": rouge_score,\n",
    "    \"qmsum\": rouge_score,\n",
    "    \"multi_news\": rouge_score,\n",
    "    \"vcsum\": rouge_zh_score,\n",
    "    \"trec\": classification_score,\n",
    "    \"triviaqa\": qa_f1_score,\n",
    "    \"samsum\": rouge_score,\n",
    "    \"lsht\": classification_score,\n",
    "    \"passage_retrieval_en\": retrieval_score,\n",
    "    \"passage_count\": count_score,\n",
    "    \"passage_retrieval_zh\": retrieval_zh_score,\n",
    "    \"lcc\": code_sim_score,\n",
    "    \"repobench-p\": code_sim_score,\n",
    "}\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--models_dir', type=str, required=True, help=\"Directory containing model directories\")\n",
    "    parser.add_argument('--output_dir', type=str, required=True, help=\"Directory to save evaluation results\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def scorer(dataset, predictions, answers, all_classes):\n",
    "    total_score = 0.\n",
    "    for (prediction, ground_truths) in zip(predictions, answers):\n",
    "        score = 0.\n",
    "        if dataset in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\"]:\n",
    "            prediction = prediction.lstrip('\\n').split('\\n')[0]\n",
    "        for ground_truth in ground_truths:\n",
    "            score = max(score, dataset2metric[dataset](prediction.lower(), ground_truth.lower(), all_classes=all_classes))\n",
    "        total_score += score\n",
    "    return round(100 * total_score / len(predictions), 2)\n",
    "\n",
    "def eval(path):\n",
    "    scores = dict()\n",
    "    if not path.endswith('/'):\n",
    "        path += '/'\n",
    "    \n",
    "    all_files = os.listdir(path)\n",
    "    print(\"Evaluating on:\", all_files)\n",
    "\n",
    "    for filename in all_files:\n",
    "        if not filename.endswith(\"jsonl\"):\n",
    "            continue\n",
    "        \n",
    "        predictions, answers, lengths, all_classes = [], [], [], []\n",
    "        dataset = filename.split('.')[0]\n",
    "        \n",
    "        with open(f\"{path}{filename}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                if \"context_length\" in data and data[\"context_length\"] < 35000:\n",
    "                    predictions.append(data[\"pred\"])\n",
    "                    answers.append(data[\"answers\"])\n",
    "                    if \"all_classes\" in data:\n",
    "                        all_classes = data[\"all_classes\"]\n",
    "                    if \"length\" in data:\n",
    "                        lengths.append(data[\"length\"])\n",
    "                elif data[\"length\"] <= 20000:\n",
    "                    predictions.append(data[\"pred\"])\n",
    "                    answers.append(data[\"answers\"])\n",
    "                    if \"all_classes\" in data:\n",
    "                        all_classes = data[\"all_classes\"]\n",
    "                    if \"length\" in data:\n",
    "                        lengths.append(data[\"length\"])\n",
    "\n",
    "        score = scorer(dataset, predictions, answers, all_classes)\n",
    "        scores[dataset] = score\n",
    "        print(f\"{dataset}: {score}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/checkpoint-500-fuyao...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 4.7\n",
      "musique: 4.38\n",
      "multifieldqa_en: 15.24\n",
      "hotpotqa: 9.17\n",
      "2wikimqa: 9.39\n",
      "qasper: 10.37\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/250_short_Orca...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 3.31\n",
      "musique: 3.88\n",
      "multifieldqa_en: 17.99\n",
      "hotpotqa: 7.48\n",
      "2wikimqa: 10.65\n",
      "qasper: 7.81\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/token_version_6.7_checkpoint-250_dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 15.55\n",
      "musique: 18.28\n",
      "multifieldqa_en: 28.56\n",
      "hotpotqa: 27.62\n",
      "2wikimqa: 25.25\n",
      "qasper: 21.92\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/ck-250-version7...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 3.59\n",
      "musique: 3.9\n",
      "multifieldqa_en: 20.18\n",
      "hotpotqa: 8.28\n",
      "2wikimqa: 9.31\n",
      "qasper: 8.4\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/tinyllama_base_dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 9.24\n",
      "musique: 3.46\n",
      "multifieldqa_en: 15.47\n",
      "hotpotqa: 6.85\n",
      "2wikimqa: 8.48\n",
      "qasper: 6.85\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/token_version_6.7_checkpoint-124_dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 15.43\n",
      "musique: 16.12\n",
      "multifieldqa_en: 29.51\n",
      "hotpotqa: 25.15\n",
      "2wikimqa: 24.48\n",
      "qasper: 20.02\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/version_7_500step_dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 16.3\n",
      "musique: 17.92\n",
      "multifieldqa_en: 30.86\n",
      "hotpotqa: 24.79\n",
      "2wikimqa: 21.94\n",
      "qasper: 23.15\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/token_short_slim_dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 15.47\n",
      "musique: 13.57\n",
      "multifieldqa_en: 27.12\n",
      "hotpotqa: 24.36\n",
      "2wikimqa: 27.11\n",
      "qasper: 23.01\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/TinyLlama-1.1B-intermediate-step-1431k-3T...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 3.94\n",
      "musique: 2.71\n",
      "multifieldqa_en: 16.28\n",
      "hotpotqa: 7.92\n",
      "2wikimqa: 8.89\n",
      "qasper: 6.06\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/tinyllam_16K_125_step_dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 15.6\n",
      "musique: 16.53\n",
      "multifieldqa_en: 31.04\n",
      "hotpotqa: 25.94\n",
      "2wikimqa: 23.11\n",
      "qasper: 23.96\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/tinyllam_32K_500_step-dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 15.21\n",
      "musique: 16.19\n",
      "multifieldqa_en: 30.34\n",
      "hotpotqa: 27.92\n",
      "2wikimqa: 23.41\n",
      "qasper: 23.19\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/short_Slim_250_dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 14.13\n",
      "musique: 10.79\n",
      "multifieldqa_en: 29.54\n",
      "hotpotqa: 21.78\n",
      "2wikimqa: 21.48\n",
      "qasper: 19.46\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/ck-500-version7...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 3.84\n",
      "musique: 4.01\n",
      "multifieldqa_en: 22.72\n",
      "hotpotqa: 7.76\n",
      "2wikimqa: 9.93\n",
      "qasper: 8.55\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/short_Orca_250_dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 13.41\n",
      "musique: 11.44\n",
      "multifieldqa_en: 31.74\n",
      "hotpotqa: 21.34\n",
      "2wikimqa: 21.64\n",
      "qasper: 18.55\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/token_short_slim...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 3.98\n",
      "musique: 3.77\n",
      "multifieldqa_en: 13.77\n",
      "hotpotqa: 7.48\n",
      "2wikimqa: 8.45\n",
      "qasper: 7.04\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/250_step_short_slim...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 5.65\n",
      "musique: 3.58\n",
      "multifieldqa_en: 13.76\n",
      "hotpotqa: 7.49\n",
      "2wikimqa: 8.3\n",
      "qasper: 6.85\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/checkpoint-250-fuyao...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 4.26\n",
      "musique: 4.27\n",
      "multifieldqa_en: 14.87\n",
      "hotpotqa: 8.05\n",
      "2wikimqa: 9.68\n",
      "qasper: 8.5\n",
      "Evaluating model in /home/yuhao/FILM/real_world_long/LongBench/pred/eval/fuyao_tinyllama_250_dolly...\n",
      "Evaluating on: ['narrativeqa.jsonl', 'musique.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'qasper.jsonl']\n",
      "narrativeqa: 14.64\n",
      "musique: 13.19\n",
      "multifieldqa_en: 28.04\n",
      "hotpotqa: 21.12\n",
      "2wikimqa: 18.04\n",
      "qasper: 20.17\n",
      "Sorted scores saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "# 设定模型目录和输出目录\n",
    "models_dir = \"/home/yuhao/FILM/real_world_long/LongBench/pred/\"  # 替换为你的模型目录路径\n",
    "output_dir = \"/home/yuhao/FILM/real_world_long/LongBench/pred/\"  # 替换为你的输出目录路径\n",
    "\n",
    "results = {}\n",
    "model_dirs = [os.path.join(models_dir, model_dir) for model_dir in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, model_dir))]\n",
    "\n",
    "for model_dir in model_dirs:\n",
    "    model_name = os.path.basename(model_dir)\n",
    "    print(f\"Evaluating model in {model_dir}...\")\n",
    "    model_scores = eval(model_dir)\n",
    "    results[model_name] = model_scores\n",
    "\n",
    "# 将字典转换为 DataFrame 并填充缺失值\n",
    "df = pd.DataFrame.from_dict(results, orient='index').fillna(0)\n",
    "\n",
    "# 计算平均分数并添加为新列\n",
    "df['Average Score'] = df.mean(axis=1)\n",
    "\n",
    "# 按照平均分数从高到低排序 DataFrame\n",
    "df_sorted = df.sort_values(by='Average Score', ascending=False)\n",
    "\n",
    "# 保存结果到 CSV 文件\n",
    "df_sorted.to_csv(os.path.join(output_dir, 'model_scores_sorted.csv'))\n",
    "print(\"Sorted scores saved to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpred/tinyllama-context-version_6_4k-pretrain-dolly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpred_e/TinyLlama/version_4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: eval() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "eval('pred/tinyllama-context-version_6_4k-pretrain-dolly','pred_e/TinyLlama/version_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/anaconda/envs/FILM/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from metrics import (\n",
    "    qa_f1_score,\n",
    "    rouge_zh_score,\n",
    "    qa_f1_zh_score,\n",
    "    rouge_score,\n",
    "    classification_score,\n",
    "    retrieval_score,\n",
    "    retrieval_zh_score,\n",
    "    count_score,\n",
    "    code_sim_score,\n",
    ")\n",
    "\n",
    "\n",
    "dataset2metric = {\n",
    "    \"narrativeqa\": qa_f1_score,\n",
    "    \"qasper\": qa_f1_score,\n",
    "    \"multifieldqa_en\": qa_f1_score,\n",
    "    \"multifieldqa_zh\": qa_f1_zh_score,\n",
    "    \"hotpotqa\": qa_f1_score,\n",
    "    \"2wikimqa\": qa_f1_score,\n",
    "    \"musique\": qa_f1_score,\n",
    "    \"dureader\": rouge_zh_score,\n",
    "    \"gov_report\": rouge_score,\n",
    "    \"qmsum\": rouge_score,\n",
    "    \"multi_news\": rouge_score,\n",
    "    \"vcsum\": rouge_zh_score,\n",
    "    \"trec\": classification_score,\n",
    "    \"triviaqa\": qa_f1_score,\n",
    "    \"samsum\": rouge_score,\n",
    "    \"lsht\": classification_score,\n",
    "    \"passage_retrieval_en\": retrieval_score,\n",
    "    \"passage_count\": count_score,\n",
    "    \"passage_retrieval_zh\": retrieval_zh_score,\n",
    "    \"lcc\": code_sim_score,\n",
    "    \"repobench-p\": code_sim_score,\n",
    "}\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--models_dir', type=str, required=True, help=\"Directory containing model directories\")\n",
    "    parser.add_argument('--output_dir', type=str, required=True, help=\"Directory to save evaluation results\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def scorer(dataset, predictions, answers, all_classes):\n",
    "    total_score = 0.\n",
    "    for (prediction, ground_truths) in zip(predictions, answers):\n",
    "        score = 0.\n",
    "        if dataset in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\"]:\n",
    "            prediction = prediction.lstrip('\\n').split('\\n')[0]\n",
    "        for ground_truth in ground_truths:\n",
    "            score = max(score, dataset2metric[dataset](prediction.lower(), ground_truth.lower(), all_classes=all_classes))\n",
    "        total_score += score\n",
    "    return round(100 * total_score / len(predictions), 2)\n",
    "\n",
    "def eval(path):\n",
    "    scores = dict()\n",
    "    if not path.endswith('/'):\n",
    "        path += '/'\n",
    "    \n",
    "    all_files = os.listdir(path)\n",
    "    print(\"Evaluating on:\", all_files)\n",
    "\n",
    "    for filename in all_files:\n",
    "        if not filename.endswith(\"jsonl\"):\n",
    "            continue\n",
    "        \n",
    "        predictions, answers, lengths, all_classes = [], [], [], []\n",
    "        dataset = filename.split('.')[0]\n",
    "        \n",
    "        with open(f\"{path}{filename}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                if \"context_length\" in data and data[\"context_length\"] < 28000:\n",
    "                    predictions.append(data[\"pred\"])\n",
    "                    answers.append(data[\"answers\"])\n",
    "                    if \"all_classes\" in data:\n",
    "                        all_classes = data[\"all_classes\"]\n",
    "                    if \"length\" in data:\n",
    "                        lengths.append(data[\"length\"])\n",
    "                elif data[\"length\"] <= 2000:\n",
    "                    print(1)\n",
    "                    predictions.append(data[\"pred\"])\n",
    "                    answers.append(data[\"answers\"])\n",
    "                    if \"all_classes\" in data:\n",
    "                        all_classes = data[\"all_classes\"]\n",
    "                    if \"length\" in data:\n",
    "                        lengths.append(data[\"length\"])\n",
    "\n",
    "        score = scorer(dataset, predictions, answers, all_classes)\n",
    "        scores[dataset] = score\n",
    "        print(f\"{dataset}: {score}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# 设定模型目录和输出目录\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python pred.py --model /home/yuhao/gsm8k-ScRel/Save_model/dolly/token_version_6.4_Orca_dolly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tinyllama context + instruction\n",
    "67.5\n",
    "23.09\n",
    "\n",
    "tinyllama context\n",
    "\n",
    "{\n",
    "    \"triviaqa\": 64.95,\n",
    "    \"hotpotqa\": 4.48\n",
    "}\n",
    "\n",
    "\n",
    "tinyllama base\n",
    "2.45\n",
    "0.23"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FILM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
